# Повторение результатов статьи 

Повторяем эксперимент из статьи

```
Bunel, R., Hausknecht, M., Devlin, J., Singh, R., & Kohli, P. (2018). 
Leveraging grammar and reinforcement learning for neural program synthesis. 
arXiv preprint arXiv:1805.04276.
```

В статье предлагается использовать обучение с подкреплением и отсев генерируемых вариантов программ путем из их исполнения и проверки синтаксиса.

Используется сгенерированный датасет [karel_dataset](https://msr-redmond.github.io/karel-dataset/).

Исходный код для начала эксперимента доступен на [Github](https://github.com/bunelr/GandRL_for_NPS).

## Шпаргалка

См. подсказки по [сложным действиям](./FAQ.md)

## Как сдавать

 - [ ] решить задания по задачам, ответы на вопросы привести в этом файле или добавить в нужном пункте ссылку на ответ
 - [ ] оформить в виде git репозитория, в котором размещены все результаты
 - [ ] прислать ссылку на репозиторий письмом
 
Пример оформления ответа

 - как оформить ответ на вопрос?
> Например вот так
 - как добавить ответ ссылкой
> вот [ссылка](./TASK.md#как-сдавать)


Для обучения моделей можно использовать Google Colab c GPU.

## Задача 1. Подготовка данных (2 балла)

### Задание 1. (1 балл)

Получите karel_dataset, найдите в нем train.json и разберитесь в его структуре.

Вопросы
 - как правильно называется формат файла train.json?
 > JavaScript Object Notation
 - как взять часть из файла train.json?
 > При взятии части важно не создать _дополнительного_ перекоса в данных, так что выбирать элементы из датасета для создания под-сета желательно по равномерному закону распределения случайных чисел (случайно выбрать k элементов из датасета).
 >
 > Я приведу 2 простых способа. Первый - читать файл построчно, строка за строкой (в RAM, при этом, хранить не весь файл, а только некоторый _буфер_), записывая в под-сет каждую k-ую. Второй - читать не весь файл, а только его части по рассчитанным смещениям (узнать размер файла в байтах и рассчитать позиции, с которых можно начинать читать только интересующие строки), но есть заметный минус - вероятность того, что по рассчитанному смещению начинается новая строка, весьма мала, так что (чтобы не добавлять _мусор_), вероятно, в считанном буфере придётся искать знаки перехода на новую строку (2 шт., так как интересующая строка между ними). В обоих описанных случаях выборка получится неслучайной, но серьёзного перекоса, вероятно, удастся избежать, так как "пробы" будут взяты "по всех площади" датасета.
 >
 > Более сложный способ, есть шанс, что он даст результат лучше: 1) узнать размер файла; 2) сгенерировать как можно более случайные смещения (равномерное распределение) с исключением одинаковых; 3) выполнить сортировку смещений по возрастанию (чтобы кэширование IO работало эффективно); 4) читать файл датасета по сгенерированным смещениям (с поиском переводов строки и удалением _мусора_, если два смещения приводят на одну и ту же запись, взять следующую).
 - подготовьте файлы корректного формата размером 1%, 3%, 10% от оригинала train.json :white_check_mark:
 
### Задание 2. (1 балл)
 
Напишите код, который загружает данные из train.json
 
Вопросы
 - оцените объем необходимой RAM
> Объём RAM ~ data_size + data_separate_records_count * data_container_size, где data_size - размер данных для обучения (для 10% около 1 гигабайта), data_separate_records_count - количество _раздельно-хранимых_ записей (в разных экземплярах контейнера), data_container_size - средний размер служебной части контейнера. 
 - реализуйте загрузку в итератор словарей (паттерн итератор) :white_check_mark:
  
## Задача 2. Подготовка репозитория (5 баллов)

### Задание 1. (1 балл)

Получите GandRL_for_NPS из github и смерджите его в этот репозиторий (см. [как это сделать](./FAQ.md#как-объединить-репозитории) в FAQ.md)

Вопросы
 - на каком языке реализован?
> Python
 - что нужно сделать для установки эксперимента и зависимостей?
> Самый простой вариант - воссоздать окружение, которое было у экспериментатора. Коммит с кодом был сделан 16.02.2018 и он только один, значит, код был выложен уже после написания кода для эксперимента, в качестве исходников, прилагаемых к статье. Так что для запуска в оригинальной среде нужно было найти версии пакетов, доступных в пределах января 2018 или раньше, что и было сделано мной. Я установил Ubuntu 18.04 (она появилась позднее, зато это самый близкий LTS), pip для Python 2.7 и подобрал версии пакетов, при которых код работает так, как задумывал разработчик (версии пакетов теперь прописаны в setup.py: tqdm 4.64.1, numpy 1.14.0, torch 0.3.1, torchvision 0.2.0, matplotlib 2.1.2). Важно производить установку именно в том порядке, в котором пакеты перечислены, иначе pip в рамках решения зависимостей установит более новые версии, которые вызовут проблемы (setup.py не соблюдает должный порядок).
 - где должны быть размещены данные?
> В любом удобном месте, доступном для изменения пользователю, от имени которого происходит запуск. Конечно, нужно прописать соответствующий путь в командах скриптов train и evaluate.
 - что нужно сделать для запуска эксперимента, как указать параметры и какие значения выбрать?
> Нужно выполнить команды (здесь: https://github.com/michael2024-lw/labw_gandrl_nps/blob/master/README.md#evaluation), указанные авторами исходного исследования (сначала первичное обучение, затем fine-tune). Параметры я старался менять минимально. Приходилось менять их, в основном, лишь потому, что на видеокарте меньше видеопамяти, чем нужно для указанных размеров батча. Иногда при уменьшении батча тренировка переставала сходиться, и приходилось менять другие параметры, такие как rl_beam (размер лучевого поиска), nb_rollouts (число выборок для оценки градиента), число эпох и др.
>
> Я использовал следующие команды (подобрал параметры): `train_cmd.py --kernel_size 3 --conv_stack "64,64,64" --fc_stack "512" --tgt_embedding_size 256 --lstm_hidden_size 256 --nb_lstm_layers 2 --signal supervised --nb_ios 5 --nb_epochs 100 --optim_alg Adam --batch_size 64 --learning_rate 1e-4 --train_file <path> --val_file <path> --vocab <path> --result_folder <path> --use_cuda [--use_grammar | --learn_syntax --beta 1e-5]`, `train_cmd.py --signal rl --environment BlackBoxGeneralization --nb_rollouts 25 --init_weights <path> --nb_epochs 5 --optim_alg Adam --learning_rate 1e-5 --batch_size 8 --train_file <path> --val_file <path> --vocab <path> --result_folder <path> --use_cuda [--use_grammar | --learn_syntax --beta 1e-5]`, `train_cmd.py --signal beam_rl --environment BlackBoxGeneralization --reward_comb RenormExpected --rl_inner_batch 4 --rl_use_ref --rl_beam 8 --init_weights exps/supervised_use_grammar/Weights/best.model --nb_epochs 5 --optim_alg Adam --learning_rate 1e-5 --batch_size 8 --train_file <path> --val_file <path> --vocab <path> --result_folder <path> --use_cuda [--use_grammar | --learn_syntax --beta 1e-5]`
 - что нужно сделать для проверки обученной модели?
> Выполнить команду eval_cmd.py на тестовой выборке. (нужная команда здесь: https://github.com/michael2024-lw/labw_gandrl_nps/blob/master/README.md#evaluation)
>
> Например, вот так: `eval_cmd.py --model_weights <path> --vocabulary <path> --dataset <path> --eval_nb_ios 5 --eval_batch_size 8 --output_path <path> --top_k 10 --use_cuda [--beam_size 64] [--use_grammar]`

### Задание 2. (1 балл)
 
Загрузите 1% от train.json и остальные файлы из karel_dataset
 
Вопросы
 - зачем нужны файлы *.thdump в папке датасета?
> Они содержат кэшированные, подготовленные к использованию в обучении данные соответствующего файла датасета (тензоры torch и индексы).
 - что содержит new_vocab?
> Команды Karel (из которых будет состоять синтезируемая программа), с которыми в ходе обучения начинает ассоциироваться токен (одна команда - один токен).
 - где находится датасет для контроля и для теста?
> Для контроля - ./1m_6ex_karel/val.json; Для теста - ./1m_6ex_karel/test.json.
 - как устроен экземпляр данных для обучения?
> Он состоит из нескольких обучающих примеров входов-выходов (состояния поля Karel), корректного набора токенов в нужном порядке (желаемый выход), а также абстрактного синтаксического дерева для этого набора.

### Задание 3. (1 балл)

Проведите эксперимент с 1% данных

Вопросы
 - как указать вид модели?
> При помощи флагов `--kernel_size` (размер ядра свёртки), `--conv_stack` (количество каналов в каждой точке сверточной части сети, включая вход), `--fc_stack` (количество каналов в полносвязной сети), `--tgt_embedding_size` (размер вектора, кодирующего относительный смысл токена), `--lstm_hidden_size` (размерность скрытых состояний измерения LSTM), `--nb_lstm_layers` (количество слоёв LSTM), а так же вида функции потерь при помощи флага `--signal` (может принимать `supervised`, `rl`, либо `beam_rl`).
 - какие ошибки возникли при запуске и как вы их устранили?
> После успешной подготовки среды для python 2.7, возникало всего 2 типа ошибок: первый тип - ошибки из-за нехватки видеопамяти (у меня было всего 6 гигабайт); второй тип - переобучение модели, что приводило к генерации синтаксически неверных команд на валидации. Первый тип устраняется относительно легко - достаточно уменьшить размер батча и/или другие паратеры, влияющие на объём данных в памяти (например, rl_beam - размер лучевого поиска).
 - сколько эпох вы провели?
> Я старался провести не меньше эпох, чем в исходном варианте, чтобы увеличить шанс на нахождение более оптимальных весов (в итоге всё равно выбирается с наименьшим loss за всё время обучения), так что на первой этапе - 100, на втором - 5 (как в исходном варианте).
 - где сохранены результаты и логи эксперимента?
> В папке exps/\*/logs.txt. В репозитории сохранены все результаты обучения в полном объёме (для всех размеров данных) по пути https://github.com/michael2024-lw/labw_gandrl_nps/tree/master/train_results.
 - какого качества получен результат?
> exactmatch - 22.36, fullgeneralize - 32.68, semantic - 33.64, syntax - 100.0
 
3а. Оператор m += 1 имеет другое значение, чем m = m + 1. Также воспользуйтесь явным приведением типов
 
3б. Из-за разных версий torch может потребоваться адаптировать код проекта. Например, для получения значения из тензора размером 1 нужно вызывать ``item()`` вместо ``[0]``

3в. Удалите кэши данных перед запуском

3г. Попробуйте заменить tqdm на progressbar2, см. [вот это сообщение](https://github.com/tqdm/tqdm/issues/613) или установить версию 2018 года.

3д. После обновления кода не забывайте установить повторно через setup.py

 
### Задание 4. (1 балл)
 
Сделайте свой клон репозитория с исправлениями, добавьте архивы подготовленных данных 1%, 3%, 10% на файлообменник с доступом по прямой ссылке
> https://1drv.ms/f/s!ApBW31nGS260gQQpOTQFI3R_sMPC
>
>train1.json.xz:  https://9bqnyg.dm.files.1drv.com/y4mzbAoBFO-HKhAcSE-vjXaoRIpbb111wG4puvTD6uk777n2qaJeGpOoRIn-lA5ovhKOqK784nBgiTw8p2xpN7YnmeI0i0SKCWNtjSSSIWyRvQn-1FTVADfyPHU46x1n3sG3sikPpKZ4DsDZ6S9vMyERKy3k4Fy1ITOf_UfmOSwZ3d4aUIQT958FmKg3tommi2MHmW5jFF3ec4H0zrf69RY9w
>
>train3.json.xz:  https://9bqnyg.dm.files.1drv.com/y4mZYGQBIA7gqZkpQtZUug9U8tBoO_Zp1ReI7qIj8xuG-d6_tw46kFcL5F19lyBqAdNYvHDhHr_oIcFN0cm2BI2lTwjZgjZGApIEn8fpZ0UQ4XZrhAx7c-Pa1WI8OWZejDW7eLuTC2qnjX48_YpgCUH3V2R9KYKRhx6sVRWonSFch7f4zveEwkjTQxFooBVPaEjm0F272eXcC2rerWlMQ_D0Q
>
>train10.json.xz:  https://9bqnyg.dm.files.1drv.com/y4m93Lmtvt-2B2_EMYvejhW0uKM66kqjNFmJy_6LZpE3Bu8zjGbPL_Hd8aAnVRW-zZr9kZha_FSWUKfB1U5YqXLBfCNaHNzOhlodmrKVrlqqn2FKUHyBC4qgGEAyS01jPd2ZiKyOYKdCyx28lnCuR4IufW4nqx26JNL_pGra-ZYqGWqqPxD2om6LyFAA3vhnI6v5QQhKgFWK6957LTv0tGz_g
 
Вопросы
 - как получить данные по ссылке из командной строки?
> Например, так: wget {url} (вместо "{url}" подставляется прямая ссылка на скачивание)
 - где находится ваш репозиторий?
> https://github.com/michael2024-lw/labw_gandrl_nps.git

### Задание 5*. (1 балл)

Подключите журналирование кривых обучения и промежуточных результатов в [TensorboardX](https://github.com/lanpa/tensorboardX). :white_check_mark:
Для этого нужно использовать SummaryWriter для сохранения промежуточных значений функции потерь внутри цикла обучения. Например, после каждого минибатча или 100 миниматчей. 

Логи сохранять в подпапку ``runs`` папки эксперимента в ``./exps/*``

Для использования установить Tensorboard и указать данную папку в качестве исходной.
  
## Задача 3. Анализ и повторение результата (3 балла)

### Задание 1. (1 балл)

Проведите эксперименты для 1%, 3%, 10% данных для MLE (supervised), RL_beam и обучаемой и предзаданной моделью синтаксиса

Вопросы
 - сколько времени заняло проведение эксперимента, какие ресурсы использовали?
> Несколько недель. Сначала пробовал Collab, но ресурсы GPU T4 закончились довольно быстро, так что работа по портированию эксперимента на python3 оказалась бесполезна. Для проведения эксперимента был подготовлен личный компьютер с GPU. Далее, этот компьютер работал суммарно около 4 суток для обучения моделей (на GPU), помимо этого, несколько суток (суммарно) было потрачено на подбор параметров для обучения, так, на подбор параметров для fine-tune для пред-заданной грамматики с лучевым поиском был запущен перебор параметров для поиска комбинации, не приводящей к переобучению. Процесс работы с экспериментом происходил на протяжении месяца.
 - какого качества удалось достичь?

| Тестируемая сеть | exactmatch | fullgeneralize | semantic | syntax |
| :--- | :---: | :---: | :---: | :---: |
|1_learn_reinforce | 14.16 | 19.68 | 20.12 | 100.0 |
|1_learn_beam | 21.12 | 29.92 | 30.8 | 100.0 |
|1_man_reinforce | 21.76 | 31.4 | 32.4 | 100.0 |
|1_man_beam | 22.36 | 32.68 | 33.64 | 100.0 |
|3_learn_reinforce | 27.0 | 40.88 | 41.96 | 100.0 |
|3_learn_beam | 29.68 | 45.08 | 46.4 | 100.0 |
|3_man_reinforce | 31.36 | 48.76 | 50.24 | 100.0 |
|3_man_beam | 31.64 | 49.84 | 51.2 | 100.0 |
|10_learn_reinforce | 35.24 | 55.28 | 56.92 | 100.0 |
|10_learn_beam | 37.32 | 59.64 | 61.56 | 100.0 |
|10_man_reinforce | 38.04 | 60.64 | 62.28 | 100.0 |
|10_man_beam | 41.44 | 65.56 | 67.68 | 100.0 |

 - приведите 10 примеров синтеза программы для karel обученной моделью :white_check_mark:

Начните с MLE и используйте обученную таким образом модель для RL в качестве начального приближения.
См. подробнее в репозитории проекта.

### Задание 2. (2 балла)

Подготовьте слайды по эксперименту: 
1. название статьи и постановка задачи (по статье, формулы) :white_check_mark:
2. использованные данные (ссылка и как готовили) :white_check_mark:
3. постановка эксперимента (репо и команды запуска) :white_check_mark:
4. таблица с результатами (сводная) :white_check_mark:

Вопросы
 - сравните с результатами для Small dataset из статьи, удалось ли повторить их результаты?
 - как влияет выбор алгоритма контроля корректности программы на качество, в зависимости от размера?


